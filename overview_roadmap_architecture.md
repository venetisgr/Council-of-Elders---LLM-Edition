# Council of Elders - LLM Edition
## Architecture Overview, Roadmap & Resources

---

## Table of Contents

1. [Project Vision](#1-project-vision)
2. [Architecture Overview](#2-architecture-overview)
3. [Core Components](#3-core-components)
4. [Data Flow](#4-data-flow)
5. [Roadmap](#5-roadmap)
6. [Resources](#6-resources)

---

## 1. Project Vision

The **Council of Elders - LLM Edition** recreates the ancient Athenian Agora as a web application where the world's leading Large Language Models gather to debate topics posed by users. Multiple LLMs â€” Claude, GPT, Gemini, Grok, and others â€” engage in structured rounds of argumentation, responding to each other's points until a **conspectus** (shared understanding or consensus) is reached.

Users bring their own API keys (never stored on our servers) and configure the debate: choosing participants, setting the topic, and tuning parameters. The result is a living philosophical arena where AI models illuminate a topic from multiple perspectives.

### Key Principles

- **User-owned keys**: API keys are provided by users per session, transmitted over HTTPS, held only in server memory during the debate, and discarded immediately after
- **Provider-agnostic**: A unified adapter layer makes it straightforward to add new LLM providers
- **Transparency**: Every step of the debate â€” prompts, responses, consensus signals â€” is visible to the user
- **Thematic immersion**: The UI evokes the Athenian Agora â€” marble textures, amphitheater layouts, philosophical gravitas

---

## 2. Architecture Overview

### 2.1 High-Level Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                        FRONTEND                              â”‚
â”‚                   React (TypeScript)                          â”‚
â”‚                                                              â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚  â”‚ API Key  â”‚  â”‚ Topic Config â”‚  â”‚    Debate Viewer       â”‚ â”‚
â”‚  â”‚  Panel   â”‚  â”‚    Panel     â”‚  â”‚  (Amphitheater UI)     â”‚ â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â”‚                                                              â”‚
â”‚         WebSocket (Socket.IO)  +  REST (HTTP)                â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                         â”‚  HTTPS
                         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                        BACKEND                               â”‚
â”‚                  Python (FastAPI)                             â”‚
â”‚                                                              â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚  â”‚   WebSocket  â”‚  â”‚    Debate       â”‚  â”‚   Consensus    â”‚ â”‚
â”‚  â”‚   Manager    â”‚  â”‚  Orchestrator   â”‚  â”‚    Engine      â”‚ â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â”‚                                                              â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚              LLM Provider Adapters                    â”‚   â”‚
â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚   â”‚
â”‚  â”‚  â”‚Anthropicâ”‚ â”‚ OpenAI â”‚ â”‚ Google â”‚ â”‚     xAI      â”‚ â”‚   â”‚
â”‚  â”‚  â”‚ (Claude)â”‚ â”‚ (GPT)  â”‚ â”‚(Gemini)â”‚ â”‚   (Grok)     â”‚ â”‚   â”‚
â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### 2.2 Tech Stack

| Layer | Technology | Rationale |
|-------|-----------|-----------|
| **Frontend** | React 18+ with TypeScript | Rich component ecosystem, strong typing, excellent for real-time UIs |
| **Styling** | Tailwind CSS + custom theme | Rapid styling with utility classes; custom Agora theme on top |
| **State Management** | Zustand or React Context | Lightweight state management for debate state, API keys, config |
| **Build Tool** | Vite | Fast development builds, excellent HMR, simpler than Webpack |
| **Backend** | Python 3.11+ with FastAPI | Async-native, excellent for I/O-bound LLM API calls, strong typing with Pydantic |
| **WebSocket** | Socket.IO (python-socketio) | Reliable WebSocket with auto-reconnect, rooms, fallback to polling |
| **HTTP Client** | httpx (async) | Async HTTP for calling LLM APIs with streaming support |
| **Package Manager** | npm (frontend), pip/poetry (backend) | Standard tooling for each ecosystem |

### 2.3 API Key Flow

```
User enters keys in browser
        â”‚
        â–¼
Keys stored in browser sessionStorage (never localStorage)
        â”‚
        â–¼
On debate start, keys sent via HTTPS POST to backend
        â”‚
        â–¼
Backend holds keys in an in-memory session dict (keyed by session ID)
        â”‚
        â–¼
Keys used to authenticate with LLM provider APIs during debate
        â”‚
        â–¼
On debate end (or session timeout), keys are deleted from server memory
```

**Security measures:**
- Keys never written to disk, logs, or any persistent storage
- Keys transmitted only over HTTPS
- Server-side session timeout (e.g., 30 min of inactivity) auto-purges keys
- Keys are scoped to a single debate session

### 2.4 Real-Time Communication

**WebSocket (via Socket.IO)** is chosen over SSE because:
- Bidirectional: the client can send control signals (pause, stop, skip) during the debate
- Socket.IO provides automatic reconnection, room support, and fallback to HTTP long-polling
- Enables broadcasting debate events to multiple observers if needed in the future

**Events emitted from server â†’ client:**
- `debate:turn_start` â€” A new speaker is about to respond
- `debate:token_stream` â€” Streaming tokens from the current speaker
- `debate:turn_end` â€” Speaker finished their response
- `debate:consensus_check` â€” Consensus evaluation result after a round
- `debate:concluded` â€” Debate has reached consensus or termination
- `debate:error` â€” An error occurred (API failure, rate limit, etc.)

**Events emitted from client â†’ server:**
- `debate:start` â€” Begin the debate with given config
- `debate:pause` / `debate:resume` â€” Pause/resume the debate
- `debate:stop` â€” Terminate the debate early

### 2.5 Session Management

- Each debate creates an **ephemeral session** on the server, identified by a UUID
- Sessions live in an in-memory dictionary (no database for MVP)
- A session contains: API keys, debate config, conversation history, current state
- Sessions expire after 30 minutes of inactivity (configurable)
- A background task periodically cleans up expired sessions

---

## 3. Core Components

### 3.1 API Key Management Panel

**Purpose:** Allow users to enter and validate their API keys for each LLM provider before starting a debate.

**Responsibilities:**
- Present input fields for each supported provider (Anthropic, OpenAI, Google, xAI)
- Validate key format client-side (basic pattern matching)
- Validate key functionality server-side (lightweight API call, e.g., list models)
- Store keys in `sessionStorage` (cleared when tab closes)
- Show validation status (valid / invalid / untested) per provider
- Allow users to select which LLMs participate based on available keys

**Technical Details:**
- React component with controlled form inputs
- Keys are masked in the UI (password-type inputs)
- Validation endpoint: `POST /api/keys/validate` â€” sends key to backend, backend makes a minimal API call to verify
- No keys are logged or persisted at any point

**Interactions:**
- Feeds validated keys and selected participants to the **Topic Input & Configuration** panel
- Keys are sent to the backend when a debate starts

### 3.2 Topic Input & Debate Configuration

**Purpose:** Allow users to set the debate topic and configure debate parameters.

**Responsibilities:**
- Text input for the debate topic/question
- Selection of participating LLMs (from those with valid keys)
- Configuration of debate parameters:
  - **Max rounds**: How many rounds before forced termination (default: 10)
  - **Speakers per round**: All participants or a subset
  - **Temperature**: How creative/varied the responses should be (default: 0.7)
  - **Max tokens per response**: Cap per individual LLM turn (default: 1024)
  - **Consensus threshold**: How strongly the models must agree to conclude (default: 0.8)
  - **Persona/role assignments**: Optional â€” assign each LLM a perspective (e.g., "argue for", "argue against", "moderate")
- Preset topic suggestions for quick starts

**Technical Details:**
- React form component with validation
- Configuration stored in Zustand store
- Sent as JSON payload with `debate:start` WebSocket event

**Interactions:**
- Receives validated keys and available LLMs from **API Key Management Panel**
- Sends full config to the **Debate Orchestrator** (via backend) on start

### 3.3 Debate Orchestrator

**Purpose:** The core engine that runs the debate â€” managing turn order, constructing prompts, coordinating LLM calls, and determining when to stop.

**Responsibilities:**
- Manage the debate lifecycle: setup â†’ rounds â†’ consensus check â†’ conclusion
- Determine speaker order each round (structured round-robin)
- Construct prompts for each LLM that include:
  - The original topic/question
  - The system prompt (persona, debate rules)
  - The full conversation history (what each participant has said)
  - Instructions to engage with the arguments of others
- Call the appropriate **LLM Provider Adapter** for each turn
- Stream responses back to the client via WebSocket
- After each round, invoke the **Consensus Detection Engine**
- Handle errors gracefully (skip a speaker if their API fails, notify the user)
- Enforce termination conditions (max rounds, consensus reached, user stop)

**Technical Details:**
- Python async class: `DebateOrchestrator`
- Runs as an async task per debate session
- Maintains debate state: round number, conversation history, participant list, consensus scores
- Uses structured prompts that evolve with the conversation:

```python
# Prompt structure for each turn
system_prompt = """
You are {model_name}, participating in a philosophical debate at the Agora.
Your role: {role_description}
Rules:
- Engage directly with points raised by other speakers
- Be substantive and cite reasoning
- If you agree with a point, say so explicitly
- If you disagree, explain why with evidence
- Aim for intellectual honesty over winning
"""

user_prompt = """
Topic: {topic}

Debate so far:
{conversation_history}

It is now your turn to speak. Respond to the arguments above.
"""
```

**Interactions:**
- Receives config from **Topic Input & Configuration** (via WebSocket event)
- Calls **LLM Provider Adapters** for each turn
- Invokes **Consensus Detection Engine** after each round
- Emits events to **Debate Viewer UI** via WebSocket
- On conclusion, triggers **Conspectus Generator**

### 3.4 LLM Provider Adapters

**Purpose:** Provide a unified interface for calling different LLM APIs, abstracting away provider-specific details.

**Responsibilities:**
- Uniform `generate(messages, config)` interface across all providers
- Handle streaming responses from each provider
- Map between our internal message format and each provider's format
- Handle rate limiting with exponential backoff
- Report token usage per call
- Graceful error handling and timeout management

**Supported Providers (MVP):**

| Provider | Models | SDK/API |
|----------|--------|---------|
| **Anthropic** | Claude Opus, Claude Sonnet | `anthropic` Python SDK |
| **OpenAI** | GPT-4o, GPT-4, o1/o3 | `openai` Python SDK |
| **Google** | Gemini Pro, Gemini Ultra | `google-genai` Python SDK |
| **xAI** | Grok | `openai` Python SDK (xAI uses OpenAI-compatible API) |

**Technical Details:**
- Abstract base class `LLMAdapter` with concrete implementations per provider
- Each adapter handles the provider-specific message format translation:

```python
class LLMAdapter(ABC):
    @abstractmethod
    async def generate_stream(
        self,
        messages: list[Message],
        config: GenerationConfig,
        api_key: str,
    ) -> AsyncGenerator[str, None]:
        """Yield tokens as they are generated."""
        ...

    @abstractmethod
    async def validate_key(self, api_key: str) -> bool:
        """Check if the API key is valid."""
        ...

class AnthropicAdapter(LLMAdapter):
    ...

class OpenAIAdapter(LLMAdapter):
    ...

class GeminiAdapter(LLMAdapter):
    ...

class XAIAdapter(LLMAdapter):
    # Uses OpenAI SDK with custom base URL
    ...
```

- Streaming is handled via async generators
- Rate limit errors trigger exponential backoff with jitter
- Token usage tracked per response for cost estimation

**Interactions:**
- Called by the **Debate Orchestrator** during each turn
- Called by the **API Key Management Panel** backend endpoint for key validation
- Called by the **Conspectus Generator** for final summary generation

### 3.5 Consensus Detection Engine

**Purpose:** Determine whether the participating LLMs have reached a sufficient level of agreement on the topic to conclude the debate.

**Responsibilities:**
- Analyze the latest round of responses for signals of agreement or disagreement
- Produce a consensus score (0.0 = total disagreement, 1.0 = full consensus)
- Detect stagnation (same arguments repeated without progress)
- Report which points are agreed upon and which remain contested

**Approach â€” Multi-Signal Consensus Detection:**

The engine uses three complementary methods:

#### Signal 1: Explicit Agreement Markers
Parse each response for phrases indicating agreement or disagreement:
- Agreement: "I agree with...", "Building on X's point...", "That's correct...", "We seem to converge on..."
- Disagreement: "I disagree...", "However, I must counter...", "That misses the point..."
- Weight these markers by position (conclusions weighted higher than caveats)

#### Signal 2: Position Extraction & Comparison
- After each round, ask a lightweight LLM call (or use the fastest available model) to extract the core positions of each participant as structured data
- Compare positions across participants using a similarity metric
- Track how positions evolve across rounds (convergence vs divergence)

```python
# Position extraction prompt
"""
Based on {speaker}'s latest response, extract their core position as a list of claims:
1. ...
2. ...
3. ...

For each claim, rate their confidence: strong / moderate / tentative
"""
```

#### Signal 3: Stagnation Detection
- If the extracted positions haven't changed significantly for 2+ rounds, flag stagnation
- If token-level similarity between consecutive responses exceeds a threshold, flag repetition

**Consensus Score Calculation:**
```
consensus_score = (
    w1 * agreement_marker_score +
    w2 * position_similarity_score +
    w3 * (1 - stagnation_penalty)
)
```
Where `w1 + w2 + w3 = 1.0` (configurable weights, defaults: 0.3, 0.5, 0.2)

**Termination Conditions (any one triggers conclusion):**
1. `consensus_score >= consensus_threshold` (configurable, default 0.8)
2. `current_round >= max_rounds`
3. Stagnation detected for 3+ consecutive rounds
4. User manually stops the debate

**Interactions:**
- Called by the **Debate Orchestrator** after each round
- Returns consensus score and analysis to the Orchestrator
- Results are emitted to the **Debate Viewer UI**

### 3.6 Debate Viewer UI

**Purpose:** Display the ongoing debate in real time with an engaging, thematically immersive interface.

**Responsibilities:**
- Show each speaker's responses as they stream in (token by token)
- Identify speakers with distinct avatars, colors, and names
- Display round progression and debate status
- Show consensus meter/indicator updating after each round
- Allow user controls: pause, resume, stop, scroll through history
- Show token usage / estimated cost per speaker
- Display the final conspectus when the debate concludes

**UI Layout â€” Amphitheater Design:**

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                  THE AGORA                         â”‚
â”‚              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                      â”‚
â”‚              â”‚  DEBATE TOPIC â”‚                      â”‚
â”‚              â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                      â”‚
â”‚                                                    â”‚
â”‚     â”Œâ”€â”€â”€â”€â”€â”                        â”Œâ”€â”€â”€â”€â”€â”        â”‚
â”‚     â”‚CLAUDEâ”‚                        â”‚ GPT â”‚        â”‚
â”‚     â”‚  ğŸ›ï¸  â”‚                        â”‚  ğŸ›ï¸  â”‚        â”‚
â”‚     â””â”€â”€â”€â”€â”€â”˜                        â””â”€â”€â”€â”€â”€â”˜        â”‚
â”‚                                                    â”‚
â”‚              â”Œâ”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”                    â”‚
â”‚              â”‚GEMINIâ”‚    â”‚GROK â”‚                    â”‚
â”‚              â”‚  ğŸ›ï¸   â”‚    â”‚  ğŸ›ï¸  â”‚                    â”‚
â”‚              â””â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”˜                    â”‚
â”‚                                                    â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚  â”‚           DEBATE TRANSCRIPT                   â”‚ â”‚
â”‚  â”‚                                               â”‚ â”‚
â”‚  â”‚  [Claude - Round 1]                           â”‚ â”‚
â”‚  â”‚  The fundamental question of...               â”‚ â”‚
â”‚  â”‚                                               â”‚ â”‚
â”‚  â”‚  [GPT-4 - Round 1]                            â”‚ â”‚
â”‚  â”‚  While I appreciate Claude's framing...       â”‚ â”‚
â”‚  â”‚                                               â”‚ â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â”‚                                                    â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚ Consensus: â–ˆâ–ˆâ–ˆâ–ˆâ–‘â–‘ 65%â”‚   â”‚ Round: 3 / 10    â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚ â¸ Pause â”‚ â¹ Stop â”‚  â”‚
â”‚                              â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Technical Details:**
- React components with Tailwind styling + custom Agora theme
- Socket.IO client for real-time updates
- Smooth auto-scroll with scroll-lock toggle
- Markdown rendering for LLM responses (react-markdown)
- Animated consensus meter
- Speaker indicators with distinct colors and icons per provider

**Interactions:**
- Receives all debate events from the backend via WebSocket
- Sends control events (pause, stop) back to the **Debate Orchestrator**
- Displays consensus data from the **Consensus Detection Engine**
- Shows the final output from the **Conspectus Generator**

### 3.7 Conspectus Generator

**Purpose:** When the debate concludes, synthesize a final summary of what was debated, where the LLMs agreed and disagreed, and what the consensus position is.

**Responsibilities:**
- Take the full debate transcript and consensus analysis as input
- Generate a structured summary:
  - **Topic**: The original question
  - **Participants**: Who debated
  - **Key arguments**: Major points raised by each side
  - **Points of agreement**: Where all/most participants converged
  - **Points of disagreement**: Where positions remain divergent
  - **Consensus position**: The synthesized shared understanding
  - **Debate statistics**: Rounds, tokens used, estimated cost

**Technical Details:**
- Uses one of the participating LLMs (user-configurable, or the fastest available) to generate the summary
- Prompt includes the full transcript + consensus engine analysis
- Output is structured markdown

```python
conspectus_prompt = """
You are a neutral scribe at the Athenian Agora. The debate on the following
topic has concluded. Produce a conspectus â€” a comprehensive summary.

Topic: {topic}
Participants: {participants}
Number of rounds: {rounds}
Final consensus score: {score}

Full transcript:
{transcript}

Consensus analysis:
{consensus_analysis}

Write a structured conspectus with these sections:
1. Overview â€” What was debated and why
2. Key Arguments â€” The strongest arguments from each participant
3. Points of Agreement â€” Where the speakers converged
4. Remaining Disagreements â€” Where they could not agree
5. Synthesis â€” The final conspectus: what can we conclude?
"""
```

**Interactions:**
- Triggered by the **Debate Orchestrator** when the debate concludes
- Uses **LLM Provider Adapters** to generate the summary
- Output is sent to the **Debate Viewer UI** for display and optional export

---

## 4. Data Flow

### 4.1 Complete Debate Lifecycle

```
1. USER SETUP
   User opens app â†’ Enters API keys â†’ Keys validated â†’ Keys stored in sessionStorage

2. DEBATE CONFIGURATION
   User enters topic â†’ Selects participants â†’ Configures parameters â†’ Clicks "Begin Debate"

3. SESSION CREATION
   Frontend sends config + keys via HTTPS â†’ Backend creates session â†’ WebSocket connection established

4. DEBATE EXECUTION (per round)
   For each participant in round:
     a. Orchestrator constructs prompt (topic + history + persona)
     b. Orchestrator calls LLM Adapter with prompt + API key
     c. Adapter streams response tokens
     d. Tokens relayed to frontend via WebSocket
     e. Full response added to conversation history

5. CONSENSUS CHECK (after each round)
   Orchestrator sends round data to Consensus Engine â†’
   Engine analyzes agreement signals â†’ Returns score â†’
   Score emitted to frontend â†’ If threshold met, proceed to step 6

6. DEBATE CONCLUSION
   Orchestrator triggers Conspectus Generator â†’
   Generator summarizes transcript â†’ Summary sent to frontend â†’
   Session keys purged from server memory

7. CLEANUP
   User can export transcript/conspectus â†’
   Session expires after timeout â†’
   All in-memory data garbage collected
```

### 4.2 Error Handling Strategy

| Error | Handling |
|-------|----------|
| **API key invalid** | Caught during validation; user prompted to re-enter |
| **Rate limit hit** | Exponential backoff (1s, 2s, 4s, 8s); notify user of delay |
| **API timeout** | Retry once; if still failing, skip speaker for this round with notice |
| **Provider outage** | Skip speaker, continue with remaining participants, notify user |
| **WebSocket disconnect** | Socket.IO auto-reconnect; debate state preserved server-side |
| **All providers fail** | Pause debate, notify user, allow retry or termination |
| **Session timeout** | Purge keys, notify user to restart if they return |

---

## 5. Roadmap

### Phase 1: Foundation & MVP

**Goal:** A working prototype where 2 LLMs debate a topic in structured rounds with a basic UI.

| Task | Details |
|------|---------|
| Project scaffolding | Set up React (Vite) frontend + FastAPI backend, monorepo structure |
| LLM Adapter: Anthropic | Implement Claude adapter with streaming |
| LLM Adapter: OpenAI | Implement GPT adapter with streaming |
| API Key Panel (basic) | Simple form for entering Anthropic and OpenAI keys |
| Topic Input (basic) | Text input for topic, basic config (max rounds) |
| Debate Orchestrator (basic) | Round-robin turn management, prompt construction, history tracking |
| WebSocket Integration | Socket.IO setup, event emission for debate turns |
| Debate Viewer (basic) | Simple transcript view with speaker labels, streaming text |
| Manual termination | User can stop the debate, see full transcript |

**Deliverable:** Two LLMs can debate a topic with streaming responses displayed in real time.

### Phase 2: Multi-LLM & Consensus

**Goal:** Support all 4 providers, add consensus detection, and produce a conspectus.

| Task | Details |
|------|---------|
| LLM Adapter: Gemini | Implement Google Gemini adapter |
| LLM Adapter: xAI | Implement Grok adapter (OpenAI-compatible) |
| Key validation | Server-side key validation for all providers |
| Full participant selection | UI for choosing which LLMs participate |
| Consensus Detection Engine | Implement multi-signal consensus scoring |
| Conspectus Generator | Post-debate summary generation |
| Debate parameters | Full config UI (temperature, max tokens, threshold, personas) |
| Token usage tracking | Count tokens per speaker, estimate costs |
| Error handling | Graceful recovery from API failures, rate limits |

**Deliverable:** Full multi-LLM debates with automatic consensus detection and summary generation.

### Phase 3: Agora Theme & UX Polish

**Goal:** Transform the UI into an immersive Agora experience with polished UX.

| Task | Details |
|------|---------|
| Agora theme design | Marble textures, amphitheater layout, Greek typography accents |
| Speaker avatars & identity | Distinct visual identity per LLM (icon, color, speaking style) |
| Animated consensus meter | Visual progress indicator with animations |
| Debate timeline | Navigable timeline showing rounds and key moments |
| Responsive design | Mobile-friendly layout |
| Debate export | Download transcript and conspectus as Markdown/PDF |
| Preset topics | Curated philosophical and contemporary debate topics |
| Toast notifications | User-friendly notifications for errors, state changes |
| Loading states & animations | Smooth transitions, speaking indicators, typing effects |

**Deliverable:** A visually polished, thematically immersive web application.

### Phase 4: Advanced Features

**Goal:** Enhanced debate modes, social features, and production readiness.

| Task | Details |
|------|---------|
| Debate modes | Socratic dialogue, Oxford-style, free-form, moderated |
| Audience/user participation | Let the user interject as a debate participant |
| Debate history (local) | Save past debates in browser localStorage |
| Shareable debates | Generate shareable links to debate transcripts |
| Cost calculator | Pre-debate cost estimation based on config |
| Additional LLM providers | Mistral, Cohere, Meta Llama (via API), DeepSeek etc. |
| Accessibility (a11y) | Screen reader support, keyboard navigation, high contrast |
| Deployment & CI/CD | Docker, cloud deployment (Vercel/Railway/Fly.io), automated tests |
| Rate limit management | Smart scheduling across providers, parallel calls where possible |

**Deliverable:** A feature-rich, production-ready application.

---

## 6. Resources

### 6.1 LLM Provider APIs

| Provider | API Documentation | Python SDK | Notes |
|----------|------------------|------------|-------|
| **Anthropic (Claude)** | [docs.anthropic.com](https://docs.anthropic.com/) | `anthropic` | Messages API with streaming |
| **OpenAI (GPT)** | [platform.openai.com/docs](https://platform.openai.com/docs/) | `openai` | Chat Completions API with streaming |
| **Google (Gemini)** | [ai.google.dev/docs](https://ai.google.dev/docs) | `google-genai` | Gemini API with streaming |
| **xAI (Grok)** | [docs.x.ai](https://docs.x.ai/) | `openai` (compatible) | Uses OpenAI-compatible API with custom base URL |

### 6.2 Key Libraries & Packages

**Backend (Python):**

| Package | Purpose |
|---------|---------|
| `fastapi` | Web framework with async support |
| `uvicorn` | ASGI server for FastAPI |
| `python-socketio` | WebSocket support (Socket.IO server) |
| `anthropic` | Anthropic/Claude API SDK |
| `openai` | OpenAI API SDK (also used for xAI) |
| `google-genai` | Google Gemini API SDK |
| `httpx` | Async HTTP client |
| `pydantic` | Data validation and settings management |
| `python-dotenv` | Environment variable management (dev only) |

**Frontend (JavaScript/TypeScript):**

| Package | Purpose |
|---------|---------|
| `react` / `react-dom` | UI framework |
| `typescript` | Type safety |
| `vite` | Build tool and dev server |
| `tailwindcss` | Utility-first CSS |
| `socket.io-client` | WebSocket client (Socket.IO) |
| `zustand` | Lightweight state management |
| `react-markdown` | Markdown rendering for LLM responses |
| `react-syntax-highlighter` | Code block highlighting in responses |
| `framer-motion` | Animations (consensus meter, transitions) |
| `lucide-react` | Icon library |

### 6.3 Development Tools

| Tool | Purpose |
|------|---------|
| `ruff` | Python linter and formatter |
| `mypy` | Python type checking |
| `pytest` | Python testing |
| `eslint` | JavaScript/TypeScript linting |
| `prettier` | Code formatting |
| `vitest` | Frontend unit testing |

### 6.4 Infrastructure

| Component | Recommendation | Notes |
|-----------|---------------|-------|
| **Frontend Hosting** | Vercel or Netlify | Free tier available, excellent for React SPAs |
| **Backend Hosting** | Railway, Fly.io, or Render | Support for WebSockets, Python, affordable |
| **Domain** | Custom domain (optional) | Can use platform subdomains for MVP |
| **SSL/TLS** | Platform-provided | All platforms above provide free HTTPS |
| **Database** | None (MVP) | In-memory sessions only; add Redis later if needed |
| **Monitoring** | Sentry (free tier) | Error tracking for production |

### 6.5 Project Structure

```
Council-of-Elders---LLM-Edition/
â”œâ”€â”€ frontend/                      # React application
â”‚   â”œâ”€â”€ src/
â”‚   â”‚   â”œâ”€â”€ components/
â”‚   â”‚   â”‚   â”œâ”€â”€ ApiKeyPanel/       # API key input and validation
â”‚   â”‚   â”‚   â”œâ”€â”€ DebateConfig/      # Topic and parameter configuration
â”‚   â”‚   â”‚   â”œâ”€â”€ DebateViewer/      # Main debate display
â”‚   â”‚   â”‚   â”œâ”€â”€ ConsensusMeter/    # Consensus visualization
â”‚   â”‚   â”‚   â”œâ”€â”€ Conspectus/        # Final summary display
â”‚   â”‚   â”‚   â””â”€â”€ common/            # Shared UI components
â”‚   â”‚   â”œâ”€â”€ hooks/                 # Custom React hooks
â”‚   â”‚   â”œâ”€â”€ stores/                # Zustand state stores
â”‚   â”‚   â”œâ”€â”€ services/              # API and WebSocket client services
â”‚   â”‚   â”œâ”€â”€ types/                 # TypeScript type definitions
â”‚   â”‚   â”œâ”€â”€ styles/                # Global styles and Agora theme
â”‚   â”‚   â”œâ”€â”€ App.tsx
â”‚   â”‚   â””â”€â”€ main.tsx
â”‚   â”œâ”€â”€ public/                    # Static assets (icons, textures)
â”‚   â”œâ”€â”€ index.html
â”‚   â”œâ”€â”€ package.json
â”‚   â”œâ”€â”€ tsconfig.json
â”‚   â”œâ”€â”€ tailwind.config.js
â”‚   â””â”€â”€ vite.config.ts
â”‚
â”œâ”€â”€ backend/                       # FastAPI application
â”‚   â”œâ”€â”€ app/
â”‚   â”‚   â”œâ”€â”€ main.py               # FastAPI app + Socket.IO mount
â”‚   â”‚   â”œâ”€â”€ config.py             # App configuration
â”‚   â”‚   â”œâ”€â”€ models/               # Pydantic models
â”‚   â”‚   â”‚   â”œâ”€â”€ debate.py         # Debate, Round, Message models
â”‚   â”‚   â”‚   â””â”€â”€ providers.py      # Provider config models
â”‚   â”‚   â”œâ”€â”€ adapters/             # LLM provider adapters
â”‚   â”‚   â”‚   â”œâ”€â”€ base.py           # Abstract base adapter
â”‚   â”‚   â”‚   â”œâ”€â”€ anthropic.py      # Claude adapter
â”‚   â”‚   â”‚   â”œâ”€â”€ openai_adapter.py # GPT adapter
â”‚   â”‚   â”‚   â”œâ”€â”€ gemini.py         # Gemini adapter
â”‚   â”‚   â”‚   â””â”€â”€ xai.py            # Grok adapter
â”‚   â”‚   â”œâ”€â”€ orchestrator/         # Debate orchestration
â”‚   â”‚   â”‚   â”œâ”€â”€ engine.py         # Main debate orchestrator
â”‚   â”‚   â”‚   â”œâ”€â”€ prompts.py        # Prompt templates
â”‚   â”‚   â”‚   â””â”€â”€ consensus.py      # Consensus detection engine
â”‚   â”‚   â”œâ”€â”€ services/             # Business logic
â”‚   â”‚   â”‚   â”œâ”€â”€ session.py        # Session management
â”‚   â”‚   â”‚   â”œâ”€â”€ conspectus.py     # Summary generation
â”‚   â”‚   â”‚   â””â”€â”€ cost.py           # Token/cost tracking
â”‚   â”‚   â”œâ”€â”€ api/                  # REST API routes
â”‚   â”‚   â”‚   â””â”€â”€ keys.py           # Key validation endpoints
â”‚   â”‚   â””â”€â”€ ws/                   # WebSocket event handlers
â”‚   â”‚       â””â”€â”€ debate.py         # Debate WebSocket events
â”‚   â”œâ”€â”€ tests/                    # Backend tests
â”‚   â”œâ”€â”€ requirements.txt
â”‚   â””â”€â”€ pyproject.toml
â”‚
â”œâ”€â”€ overview_roadmap_architecture.md   # This document
â”œâ”€â”€ TODO.md                            # Project task tracker
â””â”€â”€ README.md                          # Project README
```
